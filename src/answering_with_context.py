from google import genai

from .model_prompting import prompt_model


def answer_with_context_local(question: str, context: str, system_prompt: str, model: str) -> str:
    """
    Answer a question based on the provided context using the local Ollama model.

    Args:
        question (str): The user's question.
        context (str): The context to use for answering.
        system_prompt (str): The system prompt for answering.
        model (str): The local model identifier.

    Returns:
        str: The answer generated by the model.
    """
    prompt = system_prompt.format(context=context, question=question)
    response = prompt_model(prompt, model)
    return response.strip()

def answer_with_context_cloud(question: str, context: str, system_prompt: str, model: str, client: genai.Client) -> str:    
    """
    Answer a question based on the provided context using the cloud-based model.

    Args:
        question (str): The user's question.
        context (str): The context to use for answering.
        system_prompt (str): The system prompt for answering.
        model (str): The cloud model identifier.
        client: The cloud API client.

    Returns:
        str: The answer generated by the model.
    """
    prompt = system_prompt.format(context=context, question=question)
    response = prompt_model(prompt, model, client)
    return response.strip()

def answer_with_context(question: str, context: str, system_prompt: str, model: str, client: genai.Client = None) -> str:
    """
    Combine the summaries and answer the question using the appropriate model.

    Args:
        question (str): The user's question.
        context (str): The context to use for answering.
        system_prompt (str): The system prompt for answering.
        model (str): The model identifier.
        client: The cloud API client (if using cloud answering).

    Returns:
        str: The final answer generated by the model.
    """
    if client is not None:
        return answer_with_context_cloud(question, context, system_prompt, model, client)
    return answer_with_context_local(question, context, system_prompt, model)
